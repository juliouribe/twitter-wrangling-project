{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Act\n",
    "### By Julio Uribe\n",
    "\n",
    "The purpose of this project is to exapnd on wrangling abilities. In this file we will gather data about \"weRateDogs\" twitter posts from a couple of different sources: directly from Twitter API, using the provided twitter enhanced file for tweet id's, and pulling from udacity's server to look at neural net results in a tsv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: Import  Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import json\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Source File: Twitter Enhanced file and set up API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2356\n"
     ]
    }
   ],
   "source": [
    "#load file info into dataframe for tweet id's to use later\n",
    "twitter_archive = pd.read_csv('twitter-archive-enhanced.csv')\n",
    "#load tweet ids for api extraction\n",
    "tweet_ids = twitter_archive.tweet_id.values\n",
    "\n",
    "#Set up API credentials from file outside directory\n",
    "creds = []\n",
    "with open('/Users/Jules/Desktop/DAND/twitter_credentials.txt', 'r') as f:\n",
    "    creds = f.read().split(\"'\")\n",
    "consumer_key = creds[1]\n",
    "consumer_secret = creds[3]\n",
    "access_token = creds[5]\n",
    "access_secret = creds[7]\n",
    "#create auth object with keys\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "#create tweepy api object for requests\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True)\n",
    "\n",
    "print (len(tweet_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Source File: Query Twitter's API for JSON data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Twitter's API for JSON data for each tweet ID in the Twitter archive\n",
    "count = 0\n",
    "fails_dict = {}\n",
    "start = timer()\n",
    "# Save each tweet's returned JSON as a new line in a .txt file\n",
    "with open('tweet_json.txt', 'w') as outfile:\n",
    "    # Loop pauses/resumes at about 900 tweets due to api's rate limit\n",
    "    for tweet_id in tweet_ids:\n",
    "        count += 1\n",
    "        print(str(count) + \": \" + str(tweet_id))\n",
    "        try:\n",
    "            tweet = api.get_status(tweet_id, tweet_mode='extended')\n",
    "            print(\"Success\")\n",
    "            json.dump(tweet._json, outfile)\n",
    "            outfile.write('\\n')\n",
    "        except tweepy.TweepError as e:\n",
    "            print(\"Fail\")\n",
    "            fails_dict[tweet_id] = e\n",
    "            pass\n",
    "end = timer()\n",
    "print(end - start)\n",
    "print(fails_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load JSON data we got from Twitter API into a cleaner dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load twitter json file into a pandas dataframe\n",
    "tweets_json_full = pd.read_json(\"tweet_json.txt\", lines=True)\n",
    "#tweets_json_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a smaller version of tweets_json_full with only the columns we're interested in\n",
    "tweets_json = pd.DataFrame(tweets_json_full[['id', 'created_at', 'favorite_count', 'retweet_count', 'full_text', 'extended_entities']])\n",
    "#tweets_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeing if we can extract anything interesting from the extended_entities values\n",
    "# for i in range(5):\n",
    "#     print(tweets_json_full['extended_entities'][i]['media'][0]['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Source file: Use Requests Module to Load Neural Net Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv')\n",
    "with open(\"image_predictions.tsv\", 'wb') as file:\n",
    "    for chunk in r.iter_content(chunk_size=128):\n",
    "        file.write(chunk)\n",
    "#read file back in and create a df for image predictions data     \n",
    "image_predictions = pd.read_csv(\"image_predictions.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Data\n",
    "\n",
    "For this project, we have three dataframes we're currently working with: \n",
    "* twitter_archive - imported tweet info from twitter_archive_enhanced.txt provided by udacity. Has tweet IDs, tweet text, ratings, and other information.\n",
    "* tweets_json - data from twitter API containing retweets, favorited count, tweet text, and more.\n",
    "* image_predictions - results from neural net. Contains three predictions, image url, number of images, etc.\n",
    "\n",
    "The three dataframes provide info about the tweets posted from the WeRateDogs twitter profile. We'll do some assessing of the data before we merge these dataframes together. Then we'll clean the data to get the most complete data set we can.\n",
    "\n",
    "## Quality Issues\n",
    "* twitter_archive has 2356 entries, tweets_json has 2340 entries, and image_predictions has 2075 entries\n",
    "* 'None' values are strings but should be NaN values\n",
    "* In 'twitter_archive', incorrect and missing names under 'name' column: 'None', 'a', 'the', 'an', etc.\n",
    "* In 'twitter_archive', a few rating numerators are under 10 but according to the twitter profile, all ratings should be above 10\n",
    "* In 'twitter_archive', there's are rows where the 'rating_denominator' is lower or higher than 10. We need to standardize all rows to be out of 10.\n",
    "* In 'twitter_archive', timestamp column is in string format instead of datetime.\n",
    "* In 'image_predicitons', results from the neural net in p1 give us results in inconsistent lower/upper case. Need to make consistent\n",
    "* In 'tweets_json', 'id' column should be renamed to 'tweet_id' to be consistent with other two dataframes\n",
    "* In 'image_predicitons', we get invalid results from our neural net in p1 such as 'desktop_computer', 'electric_fan', 'wild_boar'.\n",
    "* In 'image_predictions', some prediction of dog breeds aren't actual dog breeds\n",
    "* From 'image_predictions', I see not all twitter posts actually have dogs in the post. We should toss these out\n",
    "* From 'image_predictions', we have tweets that do not have a dog present. We should toss out rows that do not have a dog present. Consider using three prediction values to toss out tweets?\n",
    "\n",
    "\n",
    "## Tidiness Issues\n",
    "* In 'twitter_archive', the last four columns (doggo, floofer, pupper, puppo) are not always observed and best serve as a category. We should combine these 4 columns into one\n",
    "* In 'twitter_archive', there are multiple values in 'source' column.\n",
    "* In 'tweets_json', we have multiple values in the 'extended entities' column. Clean up column values to iphone, vine, web client, etc.\n",
    "* twitter_archive has 2356 entries, tweets_json has 2340 entries, and image_predictions has 2075 entries. We'll merge later on tweet IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#twitter_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twitter_archive has 2356 entries, tweets_json has 2340 entries, and image_predictions has 2075 entries. We'll merge later\n",
    "# on tweet IDs.\n",
    "#twitter_archive.describe()\n",
    "#tweets_json.info()\n",
    "#twitter_archive.isnull().sum()\n",
    "#type(twitter_archive.timestamp[0])\n",
    "#twitter_archive.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#twitter_archive.name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#twitter_archive.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#twitter_archive.rating_numerator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets_json.isnull().sum()\n",
    "# tweets_json.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# image_predictions.isnull().sum()\n",
    "# image_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_predictions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#image_predictions.p1.value_counts()\n",
    "# image_predictions.p1.value_counts()\n",
    "#image_predictions.p3.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if it hits false multiple times, toss out row\n",
    "#image_predictions[image_predictions.p3 == 'space_shuttle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_predictions[image_predictions.p1 == 'coffee_mug'].jpg_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's a good chance that a large part of our data set doesn't actually contain dogs in the image, throwing off ratings\n",
    "#image_predictions['p1_dog'].mean(), image_predictions['p2_dog'].mean(), image_predictions['p3_dog'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# explore prediction results for tweets with more than one image. How does the neural net handle multiple images?\n",
    "# multi_pic = image_predictions[image_predictions[\"img_num\"] > 1]\n",
    "# multi_pic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets compare the average p1_dog, p2_dog, p3_dog rates from multiple images to the whole dataframe\n",
    "# multi_pic['p1_dog'].mean(), multi_pic['p2_dog'].mean(), multi_pic['p3_dog'].mean()\n",
    "# Multiple images is more likely to have a dog in it than the general dog prediciton rate from entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for duplicated values\n",
    "# twitter_archive[twitter_archive.tweet_id.duplicated()]\n",
    "# tweets_json[tweets_json.id.duplicated()]\n",
    "# image_predictions[image_predictions.tweet_id.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create copies for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies of all three dataframes\n",
    "twitter_archive_clean = twitter_archive.copy()\n",
    "tweets_json_clean = tweets_json.copy()\n",
    "image_predictions_clean = image_predictions.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal is to merge all three data sets. First we need to clean up some columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Merge and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define\n",
    "# Rename a few columns in the dataframes for consistency\n",
    "# Clean\n",
    "tweets_json_clean.rename(columns={'id':'tweet_id', 'created_at': 'timestamp'}, inplace=True)\n",
    "twitter_archive_clean.rename(columns={'text':'full_text', 'created_at': 'timestamp'}, inplace=True)\n",
    "# Test: Make sure column names are consistent when shared/overlapping\n",
    "#twitter_archive_clean.columns, tweets_json_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define\n",
    "# I'm going to delete several columns that are less interesting in the dataframes\n",
    "#Clean\n",
    "twitter_archive_clean.drop(['in_reply_to_status_id', 'in_reply_to_user_id', 'source', 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp', 'expanded_urls'], axis=1, inplace=True)\n",
    "tweets_json_clean.drop(['extended_entities'], axis=1, inplace=True)\n",
    "# Test: Make sure all the appropriate columns have been deleted\n",
    "#twitter_archive_clean.columns, tweets_json_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define\n",
    "# Merge the twitter_enhanced_clean and tweets_json_clean together using 'tweet_id'\n",
    "tweets_super_clean = tweets_json_clean.merge(twitter_archive_clean, how='inner', on='tweet_id')\n",
    "# Test: let's see what columns we have now and if the merge is doing what we want it to do\n",
    "#tweets_super_clean.head()\n",
    "#tweets_super_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2339 entries, 0 to 2338\n",
      "Data columns (total 12 columns):\n",
      "tweet_id              2339 non-null int64\n",
      "timestamp             2339 non-null datetime64[ns]\n",
      "favorite_count        2339 non-null int64\n",
      "retweet_count         2339 non-null int64\n",
      "full_text             2339 non-null object\n",
      "rating_numerator      2339 non-null int64\n",
      "rating_denominator    2339 non-null int64\n",
      "name                  2339 non-null object\n",
      "doggo                 2339 non-null object\n",
      "floofer               2339 non-null object\n",
      "pupper                2339 non-null object\n",
      "puppo                 2339 non-null object\n",
      "dtypes: datetime64[ns](1), int64(5), object(6)\n",
      "memory usage: 237.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Before we move onto our second merge, we can clean up this data set a bit more\n",
    "# Define\n",
    "# timestamp_x and timestamp_y show the same data but timestamp_x is already in the datetime format we want so we'll keep that one\n",
    "# two columns for full text as well. We'll keep the first one\n",
    "# Drop the columns\n",
    "tweets_super_clean.drop(['timestamp_y', 'full_text_y'], axis=1, inplace=True)\n",
    "# Rename the columns\n",
    "tweets_super_clean.rename(columns={'timestamp_x':'timestamp', 'full_text_x': 'full_text'}, inplace=True)\n",
    "# Test: verify our column surgery was clean and successful\n",
    "tweets_super_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'timestamp', 'favorite_count', 'retweet_count', 'full_text',\n",
       "       'rating_numerator', 'rating_denominator', 'name', 'doggo', 'floofer',\n",
       "       'pupper', 'puppo', 'jpg_url', 'img_num', 'p1', 'p1_conf', 'p1_dog',\n",
       "       'p2', 'p2_conf', 'p2_dog', 'p3', 'p3_conf', 'p3_dog'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll now merge tweets_super_clean with image_predictions_clean using tweet_ids\n",
    "tweets_super_clean = tweets_super_clean.merge(image_predictions_clean, on='tweet_id', how='inner')\n",
    "tweets_super_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2066 entries, 0 to 2065\n",
      "Data columns (total 23 columns):\n",
      "tweet_id              2066 non-null int64\n",
      "timestamp             2066 non-null datetime64[ns]\n",
      "favorite_count        2066 non-null int64\n",
      "retweet_count         2066 non-null int64\n",
      "full_text             2066 non-null object\n",
      "rating_numerator      2066 non-null int64\n",
      "rating_denominator    2066 non-null int64\n",
      "name                  1491 non-null object\n",
      "doggo                 80 non-null object\n",
      "floofer               8 non-null object\n",
      "pupper                222 non-null object\n",
      "puppo                 24 non-null object\n",
      "jpg_url               2066 non-null object\n",
      "img_num               2066 non-null int64\n",
      "p1                    2066 non-null object\n",
      "p1_conf               2066 non-null float64\n",
      "p1_dog                2066 non-null bool\n",
      "p2                    2066 non-null object\n",
      "p2_conf               2066 non-null float64\n",
      "p2_dog                2066 non-null bool\n",
      "p3                    2066 non-null object\n",
      "p3_conf               2066 non-null float64\n",
      "p3_dog                2066 non-null bool\n",
      "dtypes: bool(3), datetime64[ns](1), float64(3), int64(6), object(10)\n",
      "memory usage: 345.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# None values should be replaced with NaN's to reflect missing data\n",
    "tweets_super_clean.replace('None', np.nan, inplace=True)\n",
    "tweets_super_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10    2066\n",
       "Name: rating_denominator, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define\n",
    "# There's are rows where the 'rating_denominator' is lower or higher than 10. We need to standardize all rows to be out of 10\n",
    "# Clean\n",
    "tweets_super_clean.rating_denominator = 10\n",
    "# Test\n",
    "tweets_super_clean.rating_denominator.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tucker      10\n",
       "Penny       10\n",
       "Lucy        10\n",
       "Charlie     10\n",
       "Oliver      10\n",
       "Cooper      10\n",
       "Winston      8\n",
       "Sadie        8\n",
       "Bo           8\n",
       "Lola         8\n",
       "Daisy        7\n",
       "Toby         7\n",
       "Bella        6\n",
       "Jax          6\n",
       "Rusty        6\n",
       "Scout        6\n",
       "Dave         6\n",
       "Koda         6\n",
       "Milo         6\n",
       "Bailey       6\n",
       "Stanley      6\n",
       "Leo          5\n",
       "Chester      5\n",
       "Buddy        5\n",
       "Alfie        5\n",
       "Louis        5\n",
       "Larry        5\n",
       "Oscar        5\n",
       "Sophie       4\n",
       "Gus          4\n",
       "            ..\n",
       "Jackie       1\n",
       "Jay          1\n",
       "Chase        1\n",
       "Levi         1\n",
       "Dido         1\n",
       "Ralphson     1\n",
       "Simba        1\n",
       "Mairi        1\n",
       "Jim          1\n",
       "Rodney       1\n",
       "Jo           1\n",
       "Juckson      1\n",
       "Franq        1\n",
       "Crimson      1\n",
       "Bayley       1\n",
       "Carll        1\n",
       "Jennifur     1\n",
       "Bobb         1\n",
       "Rey          1\n",
       "Kayla        1\n",
       "Clyde        1\n",
       "Covach       1\n",
       "Malikai      1\n",
       "Goose        1\n",
       "Timber       1\n",
       "Alf          1\n",
       "Tedrick      1\n",
       "Cupid        1\n",
       "Ashleigh     1\n",
       "Perry        1\n",
       "Name: name, Length: 912, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define\n",
    "#In 'twitter_archive', correct all 'a', 'the', 'an', etc. dog names by replacing them with NaN values\n",
    "# Bad names tend to start with lowercase so we'll put all the lower case names into a list of bad_names\n",
    "bad_names = ['None']\n",
    "# put all the names into a series\n",
    "names_left = tweets_super_clean.name.value_counts()\n",
    "for i in names_left.index:\n",
    "    if i.islower():\n",
    "        bad_names.append(i)\n",
    "# iterate through our df and replace bad names with NaN values\n",
    "for name in bad_names:\n",
    "    tweets_super_clean.name.replace(name, np.nan, inplace=True)\n",
    "tweets_super_clean.name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# Look at the full text from tweets where we found no dog name and manually look over to confirm there aren't dog names\n",
    "#need_names = tweets_super_clean[tweets_super_clean.name == 'None'].full_text\n",
    "#for i in need_names:\n",
    "#    print (i)\n",
    "# Our visual assessment here doesn't catch dog names that we skiped. We have many tweets w/o a specific dog name mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doggo</th>\n",
       "      <th>floofer</th>\n",
       "      <th>pupper</th>\n",
       "      <th>puppo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>doggo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>puppo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>puppo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pupper</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2050</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2052</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2053</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2056</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2062</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2066 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      doggo floofer  pupper  puppo\n",
       "0       NaN     NaN     NaN    NaN\n",
       "1       NaN     NaN     NaN    NaN\n",
       "2       NaN     NaN     NaN    NaN\n",
       "3       NaN     NaN     NaN    NaN\n",
       "4       NaN     NaN     NaN    NaN\n",
       "5       NaN     NaN     NaN    NaN\n",
       "6       NaN     NaN     NaN    NaN\n",
       "7       NaN     NaN     NaN    NaN\n",
       "8       NaN     NaN     NaN    NaN\n",
       "9     doggo     NaN     NaN    NaN\n",
       "10      NaN     NaN     NaN    NaN\n",
       "11      NaN     NaN     NaN    NaN\n",
       "12      NaN     NaN     NaN  puppo\n",
       "13      NaN     NaN     NaN    NaN\n",
       "14      NaN     NaN     NaN  puppo\n",
       "15      NaN     NaN     NaN    NaN\n",
       "16      NaN     NaN     NaN    NaN\n",
       "17      NaN     NaN     NaN    NaN\n",
       "18      NaN     NaN     NaN    NaN\n",
       "19      NaN     NaN     NaN    NaN\n",
       "20      NaN     NaN     NaN    NaN\n",
       "21      NaN     NaN     NaN    NaN\n",
       "22      NaN     NaN     NaN    NaN\n",
       "23      NaN     NaN     NaN    NaN\n",
       "24      NaN     NaN     NaN    NaN\n",
       "25      NaN     NaN     NaN    NaN\n",
       "26      NaN     NaN     NaN    NaN\n",
       "27      NaN     NaN     NaN    NaN\n",
       "28      NaN     NaN  pupper    NaN\n",
       "29      NaN     NaN     NaN    NaN\n",
       "...     ...     ...     ...    ...\n",
       "2036    NaN     NaN     NaN    NaN\n",
       "2037    NaN     NaN     NaN    NaN\n",
       "2038    NaN     NaN     NaN    NaN\n",
       "2039    NaN     NaN     NaN    NaN\n",
       "2040    NaN     NaN     NaN    NaN\n",
       "2041    NaN     NaN     NaN    NaN\n",
       "2042    NaN     NaN     NaN    NaN\n",
       "2043    NaN     NaN     NaN    NaN\n",
       "2044    NaN     NaN     NaN    NaN\n",
       "2045    NaN     NaN     NaN    NaN\n",
       "2046    NaN     NaN     NaN    NaN\n",
       "2047    NaN     NaN     NaN    NaN\n",
       "2048    NaN     NaN     NaN    NaN\n",
       "2049    NaN     NaN     NaN    NaN\n",
       "2050    NaN     NaN     NaN    NaN\n",
       "2051    NaN     NaN     NaN    NaN\n",
       "2052    NaN     NaN     NaN    NaN\n",
       "2053    NaN     NaN     NaN    NaN\n",
       "2054    NaN     NaN     NaN    NaN\n",
       "2055    NaN     NaN     NaN    NaN\n",
       "2056    NaN     NaN     NaN    NaN\n",
       "2057    NaN     NaN     NaN    NaN\n",
       "2058    NaN     NaN     NaN    NaN\n",
       "2059    NaN     NaN     NaN    NaN\n",
       "2060    NaN     NaN     NaN    NaN\n",
       "2061    NaN     NaN     NaN    NaN\n",
       "2062    NaN     NaN     NaN    NaN\n",
       "2063    NaN     NaN     NaN    NaN\n",
       "2064    NaN     NaN     NaN    NaN\n",
       "2065    NaN     NaN     NaN    NaN\n",
       "\n",
       "[2066 rows x 4 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_super_clean.columns\n",
    "#tweets_super_clean['stage'] = tweets_super_clean.doggo + tweets_super_clean.floofer + tweets_super_clean.pupper + tweets_super_clean.puppo\n",
    "#tweets_super_clean.stage.value_counts()\n",
    "#tweets_super_clean.drop(['stage'], axis=1, inplace=True)\n",
    "tweets_super_clean.iloc[:, 8:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pupper     211\n",
       "doggo       80\n",
       "puppo       23\n",
       "floofer      7\n",
       "Name: stage, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace doggo, pupper, floofer, and puppo columns strings with 1's to stack back\n",
    "super_copy = tweets_super_clean.copy()\n",
    "#mapping = {'doggo': 1, 'pupper': 1, 'floofer': 1, 'puppo': 1}\n",
    "#super_copy.iloc[['doggo','pupper', 'floofer', 'puppop']].replace({'doggo': mapping, 'pupper': mapping, 'floofer': mapping, 'puppo': mapping}, inplace=True)\n",
    "\n",
    "# super_copy.doggo.replace('doggo', 1, inplace=True)\n",
    "# super_copy.doggo.replace('None', np.nan, inplace=True)\n",
    "# super_copy.pupper.replace('pupper', 1, inplace=True)\n",
    "# super_copy.floofer.replace('floofer', 1, inplace=True)\n",
    "# super_copy.puppo.replace('puppo', 1, inplace=True)\n",
    "#super_copy['stage'] = list(zip(super_copy.doggo, super_copy.floofer, super_copy.pupper, super_copy.puppo))\n",
    "#super_copy['stage'] = super_copy.doggo.astype(str)+super_copy.floofer.astype(str)+super_copy.pupper.astype(str)+super_copy.puppo.astype(str)\n",
    "super_copy['stage'] = super_copy.doggo.combine_first(super_copy.floofer).combine_first(super_copy.pupper).combine_first(super_copy.puppo)\n",
    "#(++super_copy.pupper+super_copy.puppo).astype(str)\n",
    "\n",
    "# for index, row in super_copy.iterrows():\n",
    "#     if row.doggo+row.pupper+row.floofer+row.puppo > 1:\n",
    "#         row.mixed = 1\n",
    "\n",
    "super_copy.stage.value_counts()\n",
    "#df['stages'] = (super_copy.iloc[:, 1:] == 1).idxmax(1)\n",
    "# type(tweets_super_clean.doggo[5])\n",
    "# tweets_super_clean.doggo[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-e556dc917350>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper_copy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "for i in super_copy.stage:\n",
    "    for j in i:\n",
    "        print (type(j))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Issues\n",
    "* In 'twitter_archive', incorrect and missing names under 'name' column: 'None', 'a', 'the', 'an', etc.\n",
    "* 'None' values are strings but should be NaN values\n",
    "* In 'twitter_archive', a few rating numerators are under 10 but according to the twitter profile, all ratings should be above 10\n",
    "* In 'twitter_archive', there's are rows where the 'rating_denominator' is lower or higher than 10. We need to standardize all rows to be out of 10.\n",
    "* ~~In 'twitter_archive', timestamp column is in string format instead of datetime.~~\n",
    "* In 'image_predicitons', results from the neural net in p1 give us results in inconsistent lower/upper case. Need to make consistent\n",
    "* ~~In 'tweets_json', 'id' column should be renamed to 'tweet_id' to be consistent with other two dataframes~~\n",
    "* In 'image_predicitons', we get invalid results from our neural net in p1 such as 'desktop_computer', 'electric_fan', 'wild_boar'.\n",
    "* In 'image_predictions', some prediction of dog breeds aren't actual dog breeds\n",
    "* From 'image_predictions', not all twitter posts actually have dogs in the post\n",
    "* From 'image_predictions', we have tweets that do not have a dog present. We should toss out rows that do not have a dog present. Consider using three prediction values to toss out tweets?\n",
    "\n",
    "\n",
    "## Tidiness Issues\n",
    "* ~~all three dataframes should be one table~~\n",
    "* ~~rename several columns before mering for consistency: full_text, tweet_id, timestamp to be the standard\n",
    "* ~~get rid of less interesting columns so that our dataframe isn't massive when fully merged\n",
    "* ~~twitter_archive has 2356 entries, tweets_json has 2340 entries, and image_predictions has 2075 entries~~\n",
    "* In 'twitter_archive', the last four columns (doggo, floofer, pupper, puppo) are not always observed and best serve as a category. We should combine these 4 columns into one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
